{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scanned 10 fishes for Carolina, let's preview them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import numpy\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client\n",
    "from numcodecs import Blosc\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files to to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start dask client and tell where we can see what it does\n",
    "client = Client()\n",
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/62242245/323100\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 300  # Increase dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 2\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin25' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Users/habi/Data')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('F:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'Zebrafish_Carolina_Muscles')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize = numpy.nan\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ringartefact(logfile):\n",
    "    \"\"\"Get the ring artefact correction from the  scan log file\"\"\"\n",
    "    ringartefactcorrection = numpy.nan\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Ring Artifact' in line:\n",
    "                ringartefactcorrection = int(line.split('=')[1])\n",
    "    return(ringartefactcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beamhardening(logfile):\n",
    "    \"\"\"Get the beamhardening correction from the  scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Hardening' in line:\n",
    "                beamhardeningcorrection = int(line.split('=')[1])\n",
    "    return(beamhardeningcorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files in the root folder\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                               recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s subfolders in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "for sample in Data[Data['Number of reconstructions'] == 0]['Sample']:\n",
    "    print('%s has not been reconstructed yet, we remove it from our data temporarily' % sample)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [get_ringartefact(log) for log in Data['LogFile']]\n",
    "Data['Beamhardening'] = [get_beamhardening(log) for log in Data['LogFile']]\n",
    "Data['ReconstructionGrayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Are the samples reconstructed equally?\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check voxel sizes (*rounded* to two after-comma values)\n",
    "# If different, spit out which values\n",
    "print('We scanned all samples with', end=' ')\n",
    "if len(Data['Voxelsize'].round(2).unique())>1:\n",
    "    print('%s different voxel sizes' % len(Data['Voxelsize'].round(2).unique()))\n",
    "    for vs in sorted(Data['Voxelsize'].round(2).unique()):\n",
    "        print('-', vs, 'um for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            if float(vs) == round(row['Voxelsize'],2):\n",
    "                print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('equal voxel size, namely %s um.' % Data['Voxelsize'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check beam hardening\n",
    "# If different, spit out which values\n",
    "print('We reconstructed all samples with', end=' ')\n",
    "if len(Data['RingartefactCorrection'].round(2).unique())>1:\n",
    "    print('%s different RAC' % len(Data['RingartefactCorrection'].round(2).unique()))\n",
    "    for rac in sorted(Data['RingartefactCorrection']):\n",
    "        print('-', rac, 'for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('equal ringartefact correction, namely %s.' % Data['RingartefactCorrection'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check beam hardening\n",
    "# If different, spit out which values\n",
    "print('We reconstructed all samples with', end=' ')\n",
    "if len(Data['Beamhardening'].round(2).unique())>1:\n",
    "    print('%s different beam-hardening correction' % len(Data['Beamhardening'].round(2).unique()))\n",
    "    for bh in sorted(Data['Beamhardening']):\n",
    "        print('-', bh, 'for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('equal beam-hardening correction, namely %s.' % Data['Beamhardening'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reconstruction gray value\n",
    "# If different, spit out which values\n",
    "print('We reconstructed all samples with', end=' ')\n",
    "if len(Data['ReconstructionGrayvalue'].round(2).unique())>1:\n",
    "    print('%s different max. grayvalue' % len(Data['ReconstructionGrayvalue']))\n",
    "    for recgv in sorted(Data['ReconstructionGrayvalue'].unique()):\n",
    "        print('-', recgv, ' for Samples ', end='')\n",
    "        for c, row in Data[Data['ReconstructionGrayvalue'] == recgv].iterrows():\n",
    "            print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('equal max. gray value, namely %s.' % Data['ReconstructionGrayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp'))) for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [imageio.imread(pip[0])\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,row in Data.iterrows():\n",
    "    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "    plt.imshow(row.PreviewImage)\n",
    "    plt.title(row.Sample)\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color='black'))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reconstructions into a zarray and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameRec'] = [os.path.join(os.path.dirname(f),\n",
    "                                      fish + '_rec.zarr') for f, fish in zip(Data['Folder'],\n",
    "                                                                             Data['Sample'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Converting reconstructions to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRec']):\n",
    "        print('%2s/%2s: Reading %s reconstructions and saving to %s' % (c + 1,\n",
    "                                                                        len(Data),\n",
    "                                                                        row['Number of reconstructions'],\n",
    "                                                                        row['OutputNameRec'][len(Root)+1:]))       \n",
    "        Reconstructions = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))\n",
    "        # Save out to rechunked and compressed zarr files.\n",
    "        # This takes a *long* time, but subsequent operations are then much quicker\n",
    "        Reconstructions.rechunk(200).to_zarr(row['OutputNameRec'],\n",
    "                                             overwrite=True,\n",
    "                                             compressor=Blosc(cname='zstd',\n",
    "                                                              clevel=5,\n",
    "                                                              shuffle=Blosc.BITSHUFFLE))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions from the as zarrays\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc='Fish %s' % row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.Middle.%s.png' % (row['Sample'],\n",
    "                                                         direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Anteroposterior' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].compute()\n",
    "            if 'Lateral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].compute()\n",
    "            if 'Dorsoventral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving middle images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.MiddleSlices.png' % row['Sample'])\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['Mid_' + direction])\n",
    "            if d==0:\n",
    "                plt.axhline(row.Size[1]//2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(row.Size[2]//2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[2]))\n",
    "            elif d==1:\n",
    "                plt.axhline(row.Size[0]//2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d]//2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(row.Size[0]//2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d]//2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' Middle slice'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_' + direction] = Reconstructions[c].max(axis=-d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving MIP images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']), row['Sample'] + '.MIPs.png')\n",
    "    if not os.path.exists(outfilepath):       \n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done with the preparation of %s scans!' % len(Data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
