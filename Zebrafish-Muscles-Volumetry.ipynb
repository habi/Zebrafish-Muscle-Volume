{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scanned 10 fishes for Carolina, let's fiddle with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask.array\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "from dask.distributed import Client\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm import notebook\n",
    "import statsmodels\n",
    "import scipy.signal\n",
    "import sklearn.cluster\n",
    "from skimage.segmentation import random_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files to to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "# Then go to http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/62242245/323100\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 300  # Increase dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 2\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform.node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin25' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Users/habi/Data')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('F:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'Zebrafish_Carolina_Muscles')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files in the root folder\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'),\n",
    "                                               recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s subfolders in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichexperiment(i):\n",
    "    '''Categorize into 'WT' and 'KO' '''\n",
    "    if 'ko' in i:\n",
    "        return 'KO'\n",
    "    if 'wt' in i:\n",
    "        return 'WT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Experiment'] = [whichexperiment(f) for f in Data['Sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichfish(i):\n",
    "    '''Give each fish a number '''\n",
    "    return int(i[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Fish'] = [whichfish(f) for f in Data['Sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    pixelsize = numpy.nan\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reconstructions\n",
    "Data['OutputNameRec'] = [os.path.join(os.path.dirname(f),\n",
    "                                      fish + '_rec.zarr') for f, fish in zip(Data['Folder'],\n",
    "                                                                             Data['Sample'])]\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc='Fish %s' % row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.Middle.%s.png' % (row['Sample'],\n",
    "                                                         direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Anteroposterior' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].compute()\n",
    "            if 'Lateral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].compute()\n",
    "            if 'Dorsoventral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_' + direction] = Reconstructions[c].max(axis=-d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose images, so the fishes are horizontal...\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Transpose anteroposterior and lateral images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions[1:]),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions[1:])):\n",
    "        Data.at[c,'Mid_' + direction] = Data.at[c,'MIP_' + direction].transpose()\n",
    "        Data.at[c,'MIP_' + direction] = Data.at[c,'MIP_' + direction].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect 'center' of the fishes\n",
    "# For this we select the sagittal center between the otholiths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otolither(img, sigma=5, threshold=180, verbose=False):\n",
    "    '''\n",
    "    Function to detect the otoliths in the axial MIPs.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # Detect peaks in smoothed image, in x- and y-direction\n",
    "    x = numpy.mean(smoothed>threshold, axis=0)\n",
    "    y = numpy.mean(smoothed>threshold, axis=1)\n",
    "    peaksx, _ = scipy.signal.find_peaks(x)\n",
    "    peaksy, _ = scipy.signal.find_peaks(y)\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(numpy.ma.masked_less(img, threshold), cmap='viridis', alpha=0.618)            \n",
    "        for p in peaksx:\n",
    "            plt.axvline(p, alpha=0.5)\n",
    "        for p in peaksy:\n",
    "                plt.axhline(p, alpha=0.5)\n",
    "        plt.axvline(numpy.mean(peaksx))\n",
    "        plt.axhline(numpy.mean(peaksy))\n",
    "        plt.show()\n",
    "    return([peaksx.tolist(), peaksy.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to estimate the otholith position\n",
    "# We use this later for cropping the head off\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Otholith_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Find Otolith position', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        Data.at[c,'Otholith_' + direction] = otolither(row['MIP_' + direction], threshold=180, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview what we found\n",
    "for d, direction in enumerate(directions):\n",
    "    for c, row in Data.iterrows():\n",
    "        plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "        plt.imshow(row['MIP_' + direction], vmax=150)\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "        plt.title(row['Sample'])\n",
    "        plt.axis('off')\n",
    "        for i in row['Otholith_' + direction][0]:\n",
    "            plt.axvline(i, c='r')\n",
    "        for i in row['Otholith_' + direction][1]:\n",
    "            plt.axhline(i, c='g')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oto(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect the start/end of the tailfin.\n",
    "    Adapted from the 'detect_minima' function from the ZMK tooth cohort notebook (https://git.io/J3qqL)\n",
    "    Ultimately based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    # Smooth the curve and look for the largest deviation\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=0.025)\n",
    "    maxima = numpy.argmax(smoothed)\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS')\n",
    "        plt.axvline(maxima, c='r', label='Maximum deviation')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headcutter(whichone, sigma=5, threshold=150, verbose=False):\n",
    "    '''\n",
    "    Function to detect where the tail is.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    img = Data['MIP_Lateral'][whichone]\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # Project average brightness\n",
    "    x = numpy.mean(smoothed>threshold, axis=0)\n",
    "    # Use only the tail-part of the fish\n",
    "    cut = get_oto(x)\n",
    "    if verbose:\n",
    "        plt.imshow(img, vmax=150)\n",
    "        plt.plot(img.shape[0]/x.max()*0.618*x,c='r')\n",
    "        plt.axvline(cut)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where we crop the head off\n",
    "Data['HeadCrop'] = [headcutter(i, verbose=False) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect the start/end of the tailfin.\n",
    "    Adapted from the 'detect_minima' function from the ZMK tooth cohort notebook (https://git.io/J3qqL)\n",
    "    Ultimately based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    # Smooth the curve and look for the largest deviation\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=0.025)\n",
    "    maxima = numpy.argmax(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS')\n",
    "        plt.axvline(maxima, c='r', label='Maximum deviation')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tailcutter(whichone, start=2000, sigma=11, verbose=False):\n",
    "    '''\n",
    "    Function to detect where the tail is.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    img = Data['MIP_Lateral'][whichone]\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # Project average brightness\n",
    "    x = numpy.mean(smoothed, axis=0)\n",
    "    # Use only the tail-part of the fish\n",
    "    cut = get_minimum(x[start:])\n",
    "    if verbose:\n",
    "        plt.imshow(img, vmax=150)\n",
    "        plt.plot(img.shape[0]/x.max()*0.618*x,c='r')\n",
    "        plt.axvline(cut+start)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(cut+start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where we crop the tail off\n",
    "Data['TailCrop'] = [tailcutter(i, verbose=False) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the locations of the crops that we found\n",
    "for c,row in Data.iterrows():\n",
    "    plt.imshow(row['MIP_Lateral'])\n",
    "    plt.axvline(row.HeadCrop, c='r')\n",
    "    plt.axvline(row.TailCrop, c='r')\n",
    "    plt.title(row.Sample)\n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually crop the reconstructions down\n",
    "ReconstructionsCrop = [rec[headcrop:tailcrop] for rec, headcrop, tailcrop in zip(Reconstructions,\n",
    "                                                                                 Data['HeadCrop'],\n",
    "                                                                                 Data['TailCrop'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some slices laterally along the fishes\n",
    "for c, rec in enumerate(ReconstructionsCrop):\n",
    "    for k,i in enumerate(range(50, rec.shape[1], 100)):\n",
    "        plt.subplot(1,len(range(50, rec.shape[1], 100)), k+1)\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "        plt.imshow(rec[:,i,:], vmax=150)\n",
    "        plt.axis('off')\n",
    "        plt.title('%s: slice %s' % (Data['Sample'][c], i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the size of the datasets\n",
    "Data['SizeCrop'] = [rec.shape for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the cropped middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_Crop_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Cropped middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc='Fish %s' % row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.Crop.Middle.%s.png' % (row['Sample'],\n",
    "                                                              direction))\n",
    "    if os.path.exists(outfilepath):\n",
    "        Data.at[c, 'Mid_Crop_' + direction] = imageio.imread(outfilepath)\n",
    "    else:\n",
    "        # Generate requested axial view\n",
    "        if 'Anteroposterior' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][Data['SizeCrop'][c][0] // 2]\n",
    "        if 'Lateral' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][:, Data['SizeCrop'][c][1] // 2, :]\n",
    "        if 'Dorsoventral' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][:, :, Data['SizeCrop'][c][2] // 2]\n",
    "        # Save the calculated 'direction' view to disk\n",
    "        imageio.imwrite(outfilepath, (Data.at[c, 'Mid_Crop_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the cropped directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_Crop_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Calculating cropped MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.Crop.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_Crop_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_Crop_' + direction] = ReconstructionsCrop[c].max(axis=-d)\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'MIP_Crop_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of all reconstructions\n",
    "# Caveat dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Data['Histogram'] = [dask.array.histogram(rec, bins=2**8, range=[0, 2**8]) for rec in ReconstructionsCrop]\n",
    "# Calculate histogram data and put only h into the dataframe, since we use it quite often below.\n",
    "# Discard the bins\n",
    "Data['Histogram'] = [h.compute() for h,b in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramclusterer(img, number_of_clusters = 5, verbose=False):\n",
    "    '''Calculate the k-means clusters\n",
    "    Speed things up with MiniBatchKMeans\n",
    "    https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "    '''\n",
    "    # Setup k-means\n",
    "    kmeans_volume_subset = sklearn.cluster.MiniBatchKMeans(number_of_clusters)\n",
    "    # Cluster the histogram into the requested numer of clusters\n",
    "    # Do this on a subset of the images, to speed things up\n",
    "    ClusteredImg = kmeans_volume_subset.fit_predict(sorted(numpy.array(img).reshape(-1,1)))\n",
    "    # Reshape image\n",
    "    ClusteredImg.shape = img.shape\n",
    "    if verbose:\n",
    "        # Calculate histogram\n",
    "        histogram, bins = dask.array.histogram(img, bins=2**8, range=[0, 2**8])\n",
    "        plt.semilogy(numpy.log(histogram), label='Gray value histogram')\n",
    "        plt.semilogy(histogram, label='Gray value histogram (log)')\n",
    "        for c, cluster in enumerate(sorted(kmeans_volume_subset.cluster_centers_.squeeze())):\n",
    "            plt.axvline(cluster, label='Cluster center %s at %0.0f' % (c,  cluster),\n",
    "                        color=seaborn.color_palette(n_colors=number_of_clusters)[c])\n",
    "        plt.legend()\n",
    "        plt.xlim([0,2**8])\n",
    "        plt.title('Logarithmic histogram of input image with %s cluster centers' % number_of_clusters)\n",
    "        plt.show()\n",
    "    return(sorted(kmeans_volume_subset.cluster_centers_.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['ClusterCenters'] = [histogramclusterer(rec[::100], verbose=False) for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms per experiment\n",
    "for c, experiment in enumerate(Data.Experiment.unique()):\n",
    "    plt.subplot(2,1,c+1)\n",
    "    plt.title(experiment)\n",
    "    for c,row in Data[Data.Experiment == experiment].iterrows():\n",
    "        plt.semilogy(row.Histogram,\n",
    "                     label=row.Sample,\n",
    "                     color=seaborn.color_palette(n_colors=len(Data))[c])\n",
    "        for cc in row.ClusterCenters:\n",
    "            plt.axvline(cc,\n",
    "                        color=seaborn.color_palette(n_colors=len(Data))[c],\n",
    "                        alpha=.616)\n",
    "    plt.xlim([0,2**8])\n",
    "    plt.legend()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Histograms.Experiment.ClusterCenters.png'))   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All histograms, colored per experiment\n",
    "for c,row in Data.iterrows():\n",
    "    color=0\n",
    "    if row.Experiment=='WT':\n",
    "        color=1\n",
    "    plt.semilogy(row.Histogram,\n",
    "                 label=row.Sample,\n",
    "                 color=seaborn.color_palette(n_colors=2)[color])\n",
    "plt.xlim([0,2**8])\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Histograms.Experiment.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 500\n",
    "for c, clustercntr in enumerate(Data['ClusterCenters']):\n",
    "    print('-----Sample %s----' % Data['Sample'][c])\n",
    "    for imgnr, image in enumerate(ReconstructionsCrop[c][::iterator]):\n",
    "        for d, threshold in enumerate(clustercntr):\n",
    "            plt.subplot(1,len(clustercntr), d+1)\n",
    "            plt.imshow(image)\n",
    "            plt.imshow(image<threshold, cmap='viridis', alpha=0.5)\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "            plt.title('%s\\nThreshold %s' % (os.path.basename(row.Reconstructions[row.HeadCrop:row.TailCrop][::iterator][imgnr]),\n",
    "                                            round(threshold, 2)))\n",
    "            plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a median filter to the cropped reconstructions\n",
    "ReconstructionsMedian = [dask_image.ndfilters.median_filter(rec, size=5) for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write median-filtered reconstructions to zarr files\n",
    "Data['OutputNameMedian'] = [os.path.join(os.path.dirname(f),\n",
    "                                         fish + '_rec_median.zarr') for f, fish in zip(Data['Folder'],\n",
    "                                                                                       Data['Sample'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out median-filtered recs to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameMedian']):\n",
    "        ReconstructionsMedian[c].rechunk(chunks=200).to_zarr(row['OutputNameMedian'],\n",
    "                                                             overwrite=True,\n",
    "                                                             compressor=Blosc(cname='zstd',\n",
    "                                                                              clevel=9,\n",
    "                                                                              shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load median-filtered reconstructions back in\n",
    "ReconstructionsMedian = [dask.array.from_zarr(file) for file in Data['OutputNameMedian']]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of median-filtered reconstructions\n",
    "Data['HistogramMedian'] = [dask.array.histogram(rec,\n",
    "                                                bins=2**8,\n",
    "                                                range=[0, 2**8]) for rec in ReconstructionsMedian]\n",
    "Data['HistogramMedian'] = [h.compute() for h,b in Data['HistogramMedian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what we did there\n",
    "whichsample = 3\n",
    "whichslice = 800\n",
    "plt.subplot(221)\n",
    "plt.imshow(ReconstructionsMedian[whichsample][whichslice])\n",
    "plt.axis('off')\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichsample], 'um'))\n",
    "plt.title('%s, median-filtered rec %s' % (Data.Sample[whichsample], whichslice))\n",
    "plt.subplot(222)\n",
    "plt.semilogy(dask.array.histogram(ReconstructionsMedian[whichsample][whichslice], bins=2**8, range=[0, 2**8])[0])\n",
    "plt.title('Logarithmic histogram')\n",
    "plt.subplot(223)\n",
    "plt.imshow(ReconstructionsCrop[whichsample][whichslice])\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichsample], 'um'))\n",
    "plt.title('%s, original rec %s' % (Data.Sample[whichsample], whichslice))\n",
    "plt.axis('off')\n",
    "plt.subplot(224)\n",
    "plt.semilogy(dask.array.histogram(ReconstructionsCrop[whichsample][whichslice], bins=2**8, range=[0, 2**8])[0])\n",
    "plt.title('Logarithmic histogram of original rec')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['ClusterCentersMedian'] = [histogramclusterer(rec[::111], verbose=False) for rec in ReconstructionsMedian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of median data per experiment\n",
    "for c, experiment in enumerate(Data.Experiment.unique()):\n",
    "    plt.subplot(2,1,c+1)\n",
    "    plt.title(experiment)\n",
    "    for c,row in Data[Data.Experiment == experiment].iterrows():\n",
    "        plt.semilogy(row.HistogramMedian,\n",
    "                     label=row.Sample,\n",
    "                     color=seaborn.color_palette(n_colors=len(Data))[c])\n",
    "        for cc in row.ClusterCentersMedian:\n",
    "            plt.axvline(cc,\n",
    "                        color=seaborn.color_palette(n_colors=len(Data))[c],\n",
    "                        alpha=.616)\n",
    "    plt.xlim([0,2**8])\n",
    "    plt.legend()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Histograms.Median.Experiment.ClusterCenters.png'))   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All median histograms, colored per experiment\n",
    "for c,row in Data.iterrows():\n",
    "    color=0\n",
    "    if row.Experiment=='WT':\n",
    "        color=1\n",
    "    plt.semilogy(row.HistogramMedian,\n",
    "                 label=row.Sample,\n",
    "                 color=seaborn.color_palette(n_colors=2)[color])\n",
    "plt.xlim([0,2**8])\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Histograms.Median.Experiment.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate peaks of all histograms, we use them for the segmentation afterwards\n",
    "Data['Peaks'] = [scipy.signal.find_peaks(h,prominence=[777, None]) for h in Data['Histogram']]\n",
    "Data['Peaks'] = [numpy.ma.masked_less(p,23).compressed() for p,details in Data['Peaks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All median histograms, colored per experiment\n",
    "lines = 2\n",
    "for c,row in Data.iterrows():\n",
    "    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "    color=0\n",
    "    if row.Experiment=='WT':\n",
    "        color=1\n",
    "    plt.semilogy(row.Histogram,\n",
    "                 label=row.Sample,\n",
    "                 color=seaborn.color_palette(n_colors=3)[0])\n",
    "    plt.semilogy(row.HistogramMedian,\n",
    "                 label=row.Sample,\n",
    "                 color=seaborn.color_palette(n_colors=3)[1])    \n",
    "    #Plot them peaks\n",
    "    for p in row.Peaks:\n",
    "        plt.axvline(p, label=p, color=seaborn.color_palette(n_colors=3)[0])\n",
    "    plt.xlim([0,2**8])\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out median filtered reconstructions   \n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Writing median-filtered reconstructions',\n",
    "                            total=len(Data)):\n",
    "    # Generate output folder\n",
    "    os.makedirs(os.path.join(os.path.dirname(row.Folder), 'rec_median'), exist_ok=True)\n",
    "    # For every reconstructions, load it's median-filtered counterpart\n",
    "    # But only do this for the relevant filenames, e.g. those between the crops :)\n",
    "    for d, name in notebook.tqdm(enumerate(row.Reconstructions[row.HeadCrop:row.TailCrop]),\n",
    "                                 total=len(ReconstructionsMedian[c]),\n",
    "                                 leave=False):\n",
    "        filename = name.replace('rec', 'rec_median')\n",
    "        if not os.path.exists(filename):\n",
    "            imageio.imwrite(filename, ReconstructionsMedian[c][d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(image, peaks, verbose=False):\n",
    "    # https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_random_walker_segmentation.html#sphx-glr-auto-examples-segmentation-plot-random-walker-segmentation-py\n",
    "    markers = numpy.zeros_like(image, dtype=numpy.uint)\n",
    "    markers[image < peaks[0]] = 1\n",
    "    markers[image > peaks[0]] = 2\n",
    "    labels = random_walker(image, markers, beta=500)\n",
    "    # 'Scale' image from 0 to 1\n",
    "    labels = (labels - 1)\n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(markers)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(labels)\n",
    "        plt.axis('off')\n",
    "    # Force image return as 8bit\n",
    "    labels = numpy.array(labels * (2**8 - 1),dtype='uint8')\n",
    "    return(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out random-walker-segmented median filtered reconstructions\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Writing random-walker segmentation', total=len(Data)):\n",
    "    # Generate output folder\n",
    "    os.makedirs(os.path.join(os.path.dirname(row.Folder), 'rec_median_segmented'), exist_ok=True)\n",
    "    # For every reconstructions, load it's median-filtered counterpart, random-walker-segment it and write it out\n",
    "    # But only do this for the relevant filenames, e.g. those between the crops :)\n",
    "    for d, name in notebook.tqdm(enumerate(row.Reconstructions[row.HeadCrop:row.TailCrop]),\n",
    "                                 desc=row.Sample,\n",
    "                                 total=len(ReconstructionsMedian[c]),\n",
    "                                 leave=False):\n",
    "        filename = name.replace('rec', 'rec_median_segmented')\n",
    "        if not os.path.exists(filename):\n",
    "            try:\n",
    "                imageio.imwrite(filename, segmentor(ReconstructionsMedian[c][d], row.Peaks))\n",
    "            except IndexError:\n",
    "                # f we're missing a peak, write out original data and let's mess with it later...\n",
    "                imageio.imwrite(filename, ReconstructionsMedian[c][d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in segmented slices and save to zarr\n",
    "Data['OutputNameSegmented'] = [os.path.join(os.path.dirname(f),\n",
    "                                      fish + '_rec_median_segmented.zarr') for f, fish in zip(Data['Folder'],\n",
    "                                                                                              Data['Sample'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Converting segmented slices to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameSegmented']):\n",
    "        print('%2s/%2s: Reading %s slices and saving to %s' % (c + 1,\n",
    "                                                               len(Data),\n",
    "                                                               row['Number of reconstructions'],\n",
    "                                                               row['OutputNameSegmented'][len(Root)+1:]))\n",
    "        Segmented = dask_image.imread.imread(os.path.join(row.Folder.replace('rec', 'rec_median_segmented'),\n",
    "                                                                '*rec*.png'))       \n",
    "        Segmented.rechunk(chunks=200).to_zarr(row['OutputNameSegmented'],\n",
    "                                              overwrite=True,\n",
    "                                              compressor=Blosc(cname='zstd',\n",
    "                                                               clevel=9,\n",
    "                                                               shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (segmented) slices from their zarr arrays\n",
    "Segmented = [dask.array.from_zarr(file) for file in Data['OutputNameSegmented']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_region(segmentation, verbose=False):\n",
    "    # Get out biggest item from https://stackoverflow.com/a/55110923/323100\n",
    "    # Also used in EAWAG/ExtractOtoliths.ipynb\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert(labels.max() != 0)  # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:]) + 1\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(segmentation)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(largestCC)\n",
    "        plt.suptitle('Largest connected component')\n",
    "        plt.show()\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ReconstructionsCrop[3][444]\n",
    "plt.imshow(img>33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract largest connected component of the segmented datasets and save out again\n",
    "t = 130\n",
    "whichsample = 9\n",
    "a = get_largest_region(Segmented[whichsample]>t, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 999\n",
    "plt.subplot(131)\n",
    "plt.imshow(ReconstructionsCrop[whichsample][slice])\n",
    "plt.subplot(132)\n",
    "plt.imshow(Segmented[whichsample][slice])\n",
    "plt.subplot(133)\n",
    "plt.imshow(a[slice])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we did there\n",
    "whichsample = 0\n",
    "whichslice = 100\n",
    "\n",
    "# Show image\n",
    "plt.imshow(Segmented[whichsample][whichslice])\n",
    "plt.imshow(dask.array.ma.masked_equal(Segmented[whichsample][whichslice], 255), alpha=0.309, cmap='viridis_r')\n",
    "\n",
    "# Output counts\n",
    "print('Background: %s px' % dask.array.ma.masked_equal(Segmented[whichsample][whichslice], 0).sum().compute())\n",
    "print('Segmented: %s px' % dask.array.ma.masked_equal(Segmented[whichsample][whichslice], 255).compute().mask.sum())\n",
    "print('Image size: %s px x %s px = '\n",
    "      '%s px - %s px segmented = '\n",
    "      '%s px background' % (Segmented[whichsample][whichslice].shape[0],\n",
    "                            Segmented[whichsample][whichslice].shape[1],\n",
    "                            Segmented[whichsample][whichslice].shape[0] * Segmented[whichsample][whichslice].shape[1],\n",
    "                            dask.array.ma.masked_equal(Segmented[whichsample][whichslice], 255).compute().mask.sum(),\n",
    "                            Segmented[whichsample][whichslice].shape[0] * Segmented[whichsample][whichslice].shape[1] - dask.array.ma.masked_equal(Segmented[whichsample][whichslice], 255).compute().mask.sum()))\n",
    "# So we can 'just' sum the masked segmented data correctly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask everything that was *not* segmented and calculate the sum of this volume\n",
    "Data['SegmentedVolume'] = [dask.array.ma.masked_equal(s, 255).compute().mask.sum() for s in Segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['VolumeCrop'] = [x*y*z for x,y,z in Data['SizeCrop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReconstructionsCrop[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to cut length of fishes\n",
    "Data['SegmentedVolume_normalized_vol'] = [vol_seg / vol_data for vol_seg, vol_data in zip(Data['SegmentedVolume'],\n",
    "                                                                                          Data['VolumeCrop'])]\n",
    "Data['SegmentedVolume_normalized_length'] = [vol_seg / zxy[0] for vol_seg, zxy in zip(Data['SegmentedVolume'],\n",
    "                                                                                      Data['Size'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert volumes to cubic mm\n",
    "Data['SegmentedVolume_mm'] = [vol_vx * (vs **3) * 1e-9 for vol_vx, vs in zip(Data['SegmentedVolume'],\n",
    "                                                                             Data['Voxelsize'])]\n",
    "Data['SegmentedVolume_normalized_vol_mm'] = [vol_vx * (vs **3) * 1e-9 for vol_vx, vs in zip(Data['SegmentedVolume_normalized_vol'],\n",
    "                                                                                            Data['Voxelsize'])]\n",
    "Data['SegmentedVolume_normalized_length_mm'] = [vol_vx * (vs **3) * 1e-9 for vol_vx, vs in zip(Data['SegmentedVolume_normalized_length'],\n",
    "                                                                                               Data['Voxelsize'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[Data.Experiment=='WT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    print(exp)\n",
    "    for d, row in Data[Data.Experiment==exp].iterrows():\n",
    "        print(d, row.SegmentedVolume_normalized_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=Data,\n",
    "                x='Experiment',\n",
    "                y='SegmentedVolume_mm',\n",
    "                saturation=1)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='SegmentedVolume_mm',\n",
    "                  s=25,\n",
    "                  linewidth=2)\n",
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    for d, row in Data[Data.Experiment==exp].iterrows():\n",
    "        plt.text(c, row.SegmentedVolume_mm, row.Sample)\n",
    "plt.ylabel('Segmented volume [mm³]')\n",
    "plt.savefig(os.path.join(OutPutDir, 'SegmentedVolume.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=Data,\n",
    "                x='Experiment',\n",
    "                y='SegmentedVolume_normalized_vol',\n",
    "                saturation=1)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='SegmentedVolume_normalized_vol',\n",
    "                  s=25,\n",
    "                  linewidth=2)\n",
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    for d, row in Data[Data.Experiment==exp].iterrows():\n",
    "        plt.text(c, row.SegmentedVolume_normalized_vol, row.Sample)\n",
    "plt.ylabel('Segmented volume, normalized to data volume')\n",
    "plt.savefig(os.path.join(OutPutDir, 'SegmentedVolume.Normalized.Volume.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data=Data,\n",
    "                x='Experiment',\n",
    "                y='SegmentedVolume_normalized_length',\n",
    "                saturation=1)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='SegmentedVolume_normalized_length',\n",
    "                  s=25,\n",
    "                  linewidth=2)\n",
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    for d, row in Data[Data.Experiment==exp].iterrows():\n",
    "        plt.text(c, row.SegmentedVolume_normalized_length, row.Sample)\n",
    "plt.ylabel('Segmented volume, normalized to data volume')\n",
    "plt.savefig(os.path.join(OutPutDir, 'SegmentedVolume.Normalized.Length.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('The %s fishes have a mean segmented volume (not normalized) of %7.3f mm³'\n",
    "          % (experiment, Data[Data.Experiment == experiment]['SegmentedVolume_mm'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell us which fish is the median one\n",
    "# https://stackoverflow.com/a/61047899/323100\n",
    "for experiment in Data.Experiment.unique():\n",
    "    print('The median fish of the %s fishes is fish %s (df index %s) and has a volume of %7.3f mm³'\n",
    "          % (experiment,\n",
    "             Data[Data['SegmentedVolume_mm'] == Data[Data.Experiment == experiment]['SegmentedVolume_mm'].median()].iloc[0]['Sample'],\n",
    "             Data[Data['SegmentedVolume_mm'] == Data[Data.Experiment == experiment]['SegmentedVolume_mm'].median()].index[0],\n",
    "             Data[Data.Experiment == experiment]['SegmentedVolume_mm'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Experiment', 'Sample', 'SegmentedVolume_mm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All %s fishes have a mean segmented volume of %0.2f mm³'\n",
    "      % (len(Data), Data['SegmentedVolume_mm'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('The %s fishes have a mean segmented volume (normalized to length) of %6.0f voxels'\n",
    "          % (experiment, int(Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length'].mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell us which fish is the median one\n",
    "# https://stackoverflow.com/a/61047899/323100\n",
    "for experiment in Data.Experiment.unique():\n",
    "    print('The median fish of the %s fishes is fish %s and has a volume of %6.0f mm³'\n",
    "          % (experiment,\n",
    "             Data[Data['SegmentedVolume_normalized_length'] == Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length'].median()].iloc[0]['Sample'],\n",
    "             Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use wt05 and k003 for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Experiment', 'Sample', 'SegmentedVolume_normalized_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Data:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write XLS sheet for Carolina\n",
    "Output = Data[['Sample', 'Folder', 'LogFile', 'Experiment', 'Fish',\n",
    "               'Voxelsize', 'Number of reconstructions',  'OutputNameRec',\n",
    "               'Size', 'HeadCrop', 'TailCrop', 'SizeCrop',  'VolumeCrop',\n",
    "               'Peaks',\n",
    "               'SegmentedVolume', 'SegmentedVolume_normalized_vol', 'SegmentedVolume_normalized_length',\n",
    "               'SegmentedVolume_mm', 'SegmentedVolume_normalized_vol_mm', 'SegmentedVolume_normalized_length_mm']]\n",
    "Output.to_excel('Data.xlsx')\n",
    "Output.to_excel(os.path.join(OutPutDir, 'Data.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saved all the asked data to %s' % OutPutDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does that make sense?\n",
    "for whichslice in range(250,2222,250):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "        plt.imshow(ReconstructionsCrop[c][whichslice])\n",
    "        plt.imshow(Segmented[c][whichslice], alpha=0.618, cmap='viridis')\n",
    "        plt.title('%s: Slice %s' % (row.Sample, whichslice))\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "        plt.axis('off')   \n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5)\n",
    "    plt.savefig(os.path.join(OutPutDir, 'SegmentedSlices%04d.png' % whichslice))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itkwidgets\n",
    "from itkwidgets import view  # 3d viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(Reconstructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_explicit_thresholds = sitk.ConnectedThreshold(img_T1,\n",
    "                                                  seedList=[(600,200)],\n",
    "                                                  lower=10,\n",
    "                                                  upper=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = sitk.ImageFileWriter()\n",
    "writer.SetFileName('out.png')\n",
    "writer.Execute(seg_explicit_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg_explicit_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
